![ActivationUnits](/images/activations.png)

# Jupyter notebook to display activation units

1. ReLU: Rectifier Linear Unit
2. LReLU: Leaky ReLU
3. ELU: Exponential Linear Unit
4. GELU: Gaussian Error Linear Unit
5. MISH: Self-Regularized Non-Monotonic Activation [Paper](https://arxiv.org/pdf/1908.08681.pdf)

```python -m notebook```
# Launching notepad server

